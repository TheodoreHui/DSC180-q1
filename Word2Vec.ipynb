{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60897544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\theod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\theod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\theod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "import string\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "beee11de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theod\\AppData\\Local\\Temp\\ipykernel_42304\\3568182671.py:14: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_json(file.read()).map(lem.lemmatize)\n",
      "C:\\Users\\theod\\AppData\\Local\\Temp\\ipykernel_42304\\3568182671.py:14: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_json(file.read()).map(lem.lemmatize)\n"
     ]
    }
   ],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def lemmatize_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    lemmatized_tokens = [lem.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return lemmatized_tokens\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        df =  pickle.load(file)\n",
    "        df['sentence'] = df['sentence'].apply(lemmatize_sentence)\n",
    "        return df\n",
    "def load_json(path):\n",
    "    with open(path, 'r') as file:\n",
    "        return pd.read_json(file.read()).map(lem.lemmatize)\n",
    "data20 = load_pickle('TrainingData/20news/df20.pkl')\n",
    "datanyt = load_pickle('TrainingData/nyt/dfnyt.pkl')\n",
    "seed20 = load_json('TrainingData/20news/seedwords.json')\n",
    "seednyt = load_json('TrainingData/nyt/seedwords.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc43db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data):\n",
    "    model = Word2Vec(sentences=data['sentence'], vector_size=256, window=8, min_count=1, workers=16, ns_exponent = 1.1)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(\"word2vec_model.bin\")\n",
    "\n",
    "    # You can also load the model later using:\n",
    "    # model = Word2Vec.load(\"word2vec_model.bin\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4192e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "model20 = train_model(data20)\n",
    "modelnyt = train_model(datanyt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "96aaf57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_vectors(seeds, model):\n",
    "      return {col:sum([model.wv[seed] for seed in seeds[col]])/len(seeds[col]) for col in seeds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93979c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_vecs20 = seed_vectors(seed20, model20)\n",
    "seed_vecsnyt = seed_vectors(seednyt, modelnyt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dbf3f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_vectors(tokens, model):\n",
    "    return [sum([model.wv[token] for token in doc])/len(doc) for doc in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9788483",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vecs20 = doc_vectors(data20['sentence'], model20)\n",
    "doc_vecsnyt = doc_vectors(datanyt['sentence'], modelnyt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e685c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1,v2)/(norm(v1)*norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d89fda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relevance(doc_vec, seed_vecs):\n",
    "    out = []\n",
    "    for doc in doc_vec:\n",
    "        relevance = {seed_vec[0]:cos_sim(seed_vec[1], doc) for seed_vec in seed_vecs.items()}\n",
    "        out.append(max(zip(relevance.values(), relevance.keys()))[1])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eb8c0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data20['pred_label'] = compute_relevance(doc_vecs20, seed_vecs20)\n",
    "datanyt['pred_label'] = compute_relevance(doc_vecsnyt, seed_vecsnyt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ca69db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(expt, pred):\n",
    "    return f1_score(expt, pred, average='macro'), f1_score(expt, pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "345d7d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.7275222438428319, 0.9273011191116509),\n",
       " (0.4882732333297365, 0.5373788268798948))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(datanyt['label'], datanyt['pred_label']), score(data20['label'], data20['pred_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef881e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c04f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "datanyt['pred_label'].describe(), datanyt['label'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "135ace29",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1065754375.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[181], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Word2Vec 0.92 0.83, 0.51 0.45\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Word2Vec 0.92 0.83, 0.51 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "56951e10",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Word2Vec' has no attribute 'load_word2vec_format'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[408], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model2 \u001b[38;5;241m=\u001b[39m Word2Vec\u001b[38;5;241m.\u001b[39mload_word2vec_format(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGoogleNews-vectors-negative300.bin\u001b[39m\u001b[38;5;124m'\u001b[39m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, norm_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Word2Vec' has no attribute 'load_word2vec_format'"
     ]
    }
   ],
   "source": [
    "model2 = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, norm_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a5ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab802eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.save(\"google_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "945a3830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x14914adad10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56a55e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "((0.7166912782367217, 0.8738613689598335)\n",
    " (0.4829792424322826, 0.5453201161071253))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
